{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thesis references"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fix the references by importing a them from the word document using : https://rintze.zelle.me/ref-extractor/\n",
    "\n",
    "the resulting bib file is placed in a  (chap_direct_from_site.bib) folder, along with a direct copy of the text from the word document (raw.txt)\n",
    "\n",
    "We keep another file \"chap_to_add.bib\", which contains any references the site missed. these get merged into chap.bib, so any changes should be made in one of the two original files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "base_path = r\"D:\\surfdrive\\Documents\\Uni manuscripts\\20230109Thesis\\Python\"\n",
    "target_path = r\"D:\\surfdrive\\Documents\\Uni manuscripts\\20230109Thesis\"\n",
    "content_filename = \"content.tex\"\n",
    "raw_text_filename = \"raw.txt\"\n",
    "bib_direct_filename = \"chap_direct_from_site.bib\"\n",
    "bib_to_add_filename = \"chap_to_add.bib\"\n",
    "bib_merge_filename = \"chapmerge.bib\"\n",
    "bib_override_filename = \"chapoverride.bib\"\n",
    "all_bibs = \"chap.bib\"\n",
    "\n",
    "folders = [\n",
    "    r\"chapter.1\",\n",
    "    r\"chapter.2\",\n",
    "    r\"chapter.3\",\n",
    "    r\"chapter.4\",\n",
    "    r\"chapter.5\",\n",
    "    r\"chapter.6\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_missed(folder):\n",
    "    bib_direct = folder + bib_direct_filename\n",
    "    bib_to_add = folder + bib_to_add_filename\n",
    "\n",
    "    bib_merge = folder + bib_merge_filename\n",
    "\n",
    "    f = open(bib_direct, \"r\", encoding=\"utf8\")\n",
    "    text_direct=f.read()\n",
    "    f.close()\n",
    "    f = open(bib_to_add, \"r\", encoding=\"utf8\")\n",
    "    text_to_add=f.read()\n",
    "    f.close()\n",
    "\n",
    "    w = open(bib_merge, \"w\", encoding=\"utf8\")\n",
    "    w.write(text_direct)\n",
    "    w.write(text_to_add)\n",
    "    w.close()\n",
    "\n",
    "def merge_multiple(paths, target, override):\n",
    "\n",
    "    w = open(target, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # the override is where any refs that need additional fixing go\n",
    "    f = open(override, \"r\", encoding=\"utf8\")\n",
    "    override_text = f.read()\n",
    "    f.close()\n",
    "    w.write(override_text)\n",
    "\n",
    "    for path in paths:\n",
    "        folder_path = base_path + \"\\\\\" + path + \"\\\\\" + bib_merge_filename\n",
    "        f = open(folder_path, \"r\", encoding=\"utf8\")\n",
    "        bibs_inner=f.read()\n",
    "        f.close()\n",
    "        w.write(bibs_inner)\n",
    "\n",
    "    w.close()\n",
    "    \n",
    "def move_tex(source, target):\n",
    "    target_tex = target + content_filename\n",
    "    bu_target_tex = target + content_filename + \".bu\"\n",
    "    source_tex = source + raw_text_filename + \".final\"\n",
    "\n",
    "    shutil.copy(target_tex, bu_target_tex)\n",
    "    shutil.copy(source_tex, target_tex)\n",
    "\n",
    "def move_bib(source, target):\n",
    "    target_bib = target + bib_merge_filename\n",
    "    bu_target_bib = target + bib_merge_filename + \".bu\"\n",
    "    source_bib = source + bib_merge_filename + \".final\"\n",
    "\n",
    "    shutil.copy(target_bib, bu_target_bib)\n",
    "    shutil.copy(source_bib, target_bib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_for_type={\n",
    "    \"@article\" : [\n",
    "        \"author\",\n",
    "        \"title\",\n",
    "        \"journal\",\n",
    "        \"volume\",\n",
    "        \"number\",\n",
    "        \"pages\",\n",
    "        \"year\",\n",
    "        \"issn\",\n",
    "        \"doi\",\n",
    "    ],\n",
    "    \"@incollection\" : [\n",
    "        \"author\",\n",
    "        \"booktitle\",\n",
    "        \"title\",\n",
    "        \"year\",\n",
    "        \"pages\",\n",
    "        \"address\",\n",
    "        \"edition\",\n",
    "        \"isbn\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "rules_for_field={\n",
    "    \"author\" : [\n",
    "        (r\"[^}{\\.,\\t A-Za-zÀ-ÖØ-öø-ÿ\\-ć’š]\", r\"\"),\n",
    "    ],\n",
    "    \"title\" : [],\n",
    "    \"year\" : [\n",
    "        (r\"[^\\d,\\t ]\", r\"\"),\n",
    "    ],\n",
    "    \"pages\" : [\n",
    "        (r\"[^}{\\-\\d,\\t ]\", r\"\"),\n",
    "    ],\n",
    "    \"booktitle\" : [],\n",
    "    \"address\" : [\n",
    "        (r\"[^}{\\.,\\t A-Za-z\\(\\)]\", r\"\"),\n",
    "    ],\n",
    "    \"edition\" : [\n",
    "        (r\"[^\\d,\\t ]\", r\"\"),\n",
    "    ],\n",
    "    \"isbn\" : [\n",
    "        (r\"[^}{\\-\\d,\\t X]\", r\"\"),\n",
    "    ],\n",
    "    \"journal\" : [],\n",
    "    \"volume\" : [\n",
    "        (r\"[^\\d,\\t ]\", r\"\"),\n",
    "    ],\n",
    "    \"number\" : [\n",
    "        (r\"[^\\d,\\t ]\", r\"\"),\n",
    "    ],\n",
    "    \"issn\" : [\n",
    "        (r\"[^}{\\-\\d,\\t ]\", r\"\"),\n",
    "    ],\n",
    "    \"doi\" : [\n",
    "        (r\".*doi:\", r\" {\"),\n",
    "        (\"[A-Z]+\", lambda m: m.group(0).lower()),\n",
    "    ],\n",
    "}\n",
    "\n",
    "known_rules = [\n",
    "    (r\"{\\\\textless}\", \"<\"),\n",
    "    (r\"{\\\\textgreater}\", \">\"),\n",
    "]\n",
    "\n",
    "test = \"\"\"@article{fagerlund2017spacer,\n",
    "  journal = {Proceedings of the National Academy of Sciences},\n",
    "  doi     = {https://doi.org/doi:10.18129/B9.bioc.pepXMLTab},\n",
    "  isbn    = 1215421109,\n",
    "  issn    = {0027-8424},\n",
    "  number  = 26,\n",
    "  pmid    = 28611213,\n",
    "  title   = {Spacer capture and integration by a type I-F Cas1–Cas2-3 CRISPR adaptation complex},\n",
    "  volume  = 114,\n",
    "  author  = {Fagerlund, Robert D. and Wilkinson, Max E. and Klykov, Oleg and Barendregt, Arjan and Pearce, F. Grant and Kieper, Sebastian N. and Maxwell, Howard W. R. and Capolupo, Angela and Heck, Albert J. R. and Krause, Kurt L. and Bostina, Mihnea and Scheltema, Richard A. and Staals, Raymond H. J. and Fineran, Peter C.},\n",
    "  pages   = {5122--5128},\n",
    "  date    = 2017,\n",
    "  year    = 2017\n",
    "}\"\"\"\n",
    "\n",
    "def clean_bib_entry(bib_entry):\n",
    "    split = bib_entry.split(\"\\n\")\n",
    "    typ=split[0].split(\"{\")[0]\n",
    "    to_write = []\n",
    "    to_write.append(split[0])\n",
    "    actual_fields = split[1:-1]\n",
    "    fields = []\n",
    "    try:\n",
    "        fields = fields_for_type[typ]\n",
    "    except:\n",
    "        to_write = split\n",
    "    to_write_after = []\n",
    "    for field in fields:\n",
    "        for actual_field in actual_fields:\n",
    "            #print(field)\n",
    "            #print(actual_field)\n",
    "            found = False\n",
    "            if re.findall(field, actual_field):\n",
    "                actual_fields.remove(actual_field)\n",
    "                rules = rules_for_field[field]\n",
    "                rules.extend(known_rules)\n",
    "                for rule in rules:\n",
    "                    inner_split = actual_field.split(\"=\")\n",
    "                    value = inner_split[1]\n",
    "                    new_value = re.sub(rule[0], rule[1], value)\n",
    "                    if  value != new_value:\n",
    "                        if new_value != ',' and new_value != \"\":\n",
    "                            to_write_after.append(\"waarschuwing: changed {} from {}\".format(field, value))\n",
    "                            actual_field = inner_split[0] + \"=\" + new_value\n",
    "                        else:\n",
    "                            to_write_after.append(\"waarschuwing: fatal: changed {} from {}\".format(field, value))\n",
    "                if actual_field[-1] != \",\":\n",
    "                    actual_field = actual_field + \",\"\n",
    "\n",
    "                to_write.append(actual_field)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            to_write_after.append(\"waarschuwing: did not find {}\".format(field))\n",
    "    if split != to_write:\n",
    "        to_write.append(\"}\\n\")\n",
    "\n",
    "    a = \"\\n\".join(to_write)\n",
    "    b = \"\\n\".join(to_write_after)\n",
    "    c = \"\\n\\n\"\n",
    "\n",
    "    return a + b + c\n",
    "\n",
    "def clean_citation(cit):\n",
    "    s = cit.lower()\n",
    "    newCit = re.findall(r\"[^\\d]+\\d{4}[^,]+\", s)[0]\n",
    "    citName = re.findall(r\"[^\\d]+\", newCit)[0]\n",
    "    citTitle = re.findall(r\"\\d{4}[^,]+\", newCit)[0]\n",
    "    pats_name = [\n",
    "        (\"^de \",\"\"),\n",
    "        (\"^den \",\"\"),\n",
    "        (\"^van der \",\"\"),\n",
    "        (\"^van de \",\"\"),\n",
    "        (\"^van \",\"\"),\n",
    "        (r\"_\",r\"\"),\n",
    "        (\" \",\"\"),\n",
    "        ]\n",
    "    pats_title = [\n",
    "        (\"^(\\d+)an \",\"\\\\1\"),\n",
    "        (\"^(\\d+)a \",\"\\\\1\"),\n",
    "        (\"^(\\d+)the \",\"\\\\1\"),\n",
    "        (r\"_\",r\"\"),\n",
    "        ]\n",
    "    for pat in pats_name:\n",
    "        citName = re.sub(pat[0], pat[1], citName)\n",
    "    for pat in pats_title:\n",
    "        citTitle = re.sub(pat[0], pat[1], citTitle)\n",
    "    citTitle = re.findall(r\"\\d{4}[^\\d ]+\", citTitle)[0]\n",
    "    newCit = citName + citTitle\n",
    "    return newCit\n",
    "\n",
    "def clean_bib(cit):\n",
    "    s = cit.lower()\n",
    "    newCit = re.findall(r\"[^\\d]+\\d{4}[^,]+\", s)[0]\n",
    "    citName = re.findall(r\"[^\\d]+\", newCit)[0]\n",
    "    citTitle = re.findall(r\"\\d{4}[^,]+\", newCit)[0]\n",
    "    pats_name = [\n",
    "        (\"^de \",\"\"),\n",
    "        (\"^den \",\"\"),\n",
    "        (\"^van der \",\"\"),\n",
    "        (\"^van de \",\"\"),\n",
    "        (\"^van \",\"\"),\n",
    "        (r\"_\",r\"\"),\n",
    "        (\" \",\"\"),\n",
    "        ]\n",
    "    pats_title = [\n",
    "        (\"^(\\d+)an \",\"\\\\1\"),\n",
    "        (\"^(\\d+)a \",\"\\\\1\"),\n",
    "        (\"^(\\d+)the \",\"\\\\1\"),\n",
    "        (r\"_\",r\"\"),\n",
    "        ]\n",
    "    for pat in pats_name:\n",
    "        citName = re.sub(pat[0], pat[1], citName)\n",
    "    for pat in pats_title:\n",
    "        citTitle = re.sub(pat[0], pat[1], citTitle)\n",
    "    citTitle = re.findall(r\"\\d{4}[^\\d ]+\", citTitle)[0]\n",
    "    newCit = citName + citTitle\n",
    "    return newCit\n",
    "\n",
    "def clean_raw(raw):\n",
    "    pats= [\n",
    "        #refstuff\n",
    "        (r\"\\|\\|\\|\", r\", \"),\n",
    "\n",
    "        #remove actual comments, then replace percentages and change actual comments back\n",
    "        (r\"%%\", r\"ikwasneteencomment\"),\n",
    "        (r\"%\", r\"\\%\"),\n",
    "        (r\"ikwasneteencomment\", r\"%%\"),\n",
    "        # just escape stuff\n",
    "        (r\"_\", r\"\\_\"),\n",
    "        (r\"#\", r\"\\#\"),\n",
    "        (r\"&\", r\"\\&\"),\n",
    "\n",
    "        # special chars\n",
    "        (r\"∗\", r\"$\\\\star$\"),\n",
    "        (r\"∼\", r\"$\\\\sim$\"),\n",
    "        (r\"~\", r\"$\\\\sim$\"),\n",
    "\n",
    "        # header replacements\n",
    "        (\"\\[\\!\", r\"ikwasneteenuitroepteken\"),\n",
    "        (\"\\!\\]\", r\"ikwasneteenuitroeptekenmaardananders\"),\n",
    "        (\"\\!\\!\\!\\!(.+)\", r\"\\n\\\\paragraph{\\1}\"),\n",
    "        (\"\\!\\!\\!(.+)\", r\"\\n\\\\subsubsection{\\1}\"),\n",
    "        (\"\\!\\!(.+)\", r\"\\n\\\\subsection{\\1}\"),\n",
    "        (\"\\!(.+)\", r\"\\n\\\\section{\\1}\"),\n",
    "        (r\"ikwasneteenuitroeptekenmaardananders\", r\"!]\"),\n",
    "        (r\"ikwasneteenuitroepteken\", r\"[!\"),\n",
    "\n",
    "    ]\n",
    "\n",
    "    for pat in pats:\n",
    "        raw = re.sub(pat[0], pat[1], raw)\n",
    "    return raw\n",
    "\n",
    "def process_txt(folder):\n",
    "    txt_mapping={}\n",
    "    citeList = []\n",
    "    doneList = []\n",
    "    path = folder + raw_text_filename\n",
    "    writeFinal = path + \".final\"\n",
    "\n",
    "    f = open(path, \"r\", encoding=\"utf8\")\n",
    "    text=f.read()\n",
    "    hits = re.findall(r\"\\\\cite{([^\\}]*)}\", text)\n",
    "    for hit in hits:\n",
    "        for it in hit.split(\"|||\"):\n",
    "            citeList.append(it)\n",
    "            if \"n.d.\" in it:\n",
    "                for i in range(0, 10):\n",
    "                    print(\"bad citation:\")\n",
    "                    print(it)\n",
    "                return\n",
    "\n",
    "    for cit in set(citeList):\n",
    "        #print\n",
    "        #print(cit)\n",
    "        newCit = clean_citation(cit)\n",
    "        if newCit in doneList:\n",
    "            print(\"duplicate: {}\".format(newCit))\n",
    "        else:\n",
    "            doneList.append(newCit)\n",
    "            #wL.write(cit + \"\\n\")\n",
    "        if  newCit in txt_mapping:\n",
    "            newCit = txt_mapping[newCit]\n",
    "            print(\"alt: {}\".format(newCit))\n",
    "\n",
    "        text = text.replace(cit, newCit)\n",
    "    \n",
    "    w = open(writeFinal, \"w\", encoding=\"utf8\")\n",
    "    w.write(clean_raw(str(text)))\n",
    "    w.close()\n",
    "\n",
    "def process_bib(folder):\n",
    "    idList = []\n",
    "    \n",
    "    path = folder + bib_merge_filename\n",
    "    f = open(path, \"r\", encoding=\"utf8\")\n",
    "    text=\"\\n\" + f.read()\n",
    "    f.close()\n",
    "    writeFinal = path + \".final\"\n",
    "    wL = open(writeFinal, \"w\", encoding=\"utf8\")\n",
    "    splits = text.split(\"\\n@\")[1:]\n",
    "    for split in splits:\n",
    "        identifier = split.split(\"\\n\")[0]\n",
    "        after = re.findall(r\"[^\\d]+\\d+[^ ,\\d]+\", identifier.split('{')[1][:-1])[0].replace(\" \", \"\").lower()\n",
    "        if after in idList:\n",
    "            print(\"duplicate: {}\".format(after))\n",
    "        else:\n",
    "            #deal with illegal chars in .bib\n",
    "            to_write = (\"@\" + split.replace(identifier.split('{')[1][:-1], after)\n",
    "            .replace(\"&\", \"and\")\n",
    "            .replace(\"*\", \"\")\n",
    "            .replace(\"′\", \"\")\n",
    "            .replace(\"$\", \"\")\n",
    "            .replace(\",,\", \",\")\n",
    "            .replace(r\"_\", r\"\")\n",
    "            .replace(\"{\\\\textbackslash}\", \"\\\\\")\n",
    "            + \"\\n\")\n",
    "            to_write = clean_bib_entry(to_write)\n",
    "            wL.write(to_write)\n",
    "\n",
    "            idList.append(after)\n",
    "    wL.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate: tran2016complete\n",
      "duplicate: he2018protein\n",
      "duplicate: roepstorff1984proposal\n",
      "duplicate: shaw2020direct\n",
      "duplicate: shaw2018sequencing\n",
      "duplicate: georgiou2014promise\n",
      "duplicate: briney2019commonality\n",
      "duplicate: corti2016protective\n",
      "duplicate: georgiou2014promise\n",
      "duplicate: lavinder2014identification\n",
      "duplicate: lee2016molecular-level\n",
      "duplicate: lee2019persistent\n",
      "duplicate: he2019classification\n",
      "duplicate: he2017analysis\n",
      "duplicate: mills2015detecting\n",
      "duplicate: sharpley2019novel\n",
      "duplicate: wang2019top-down\n",
      "duplicate: guthals2017de\n",
      "duplicate: sen2017automated\n",
      "duplicate: tran2016complete\n",
      "duplicate: boer2020selectivity\n",
      "duplicate: fornelli2017top-down\n",
      "duplicate: shaw2020direct\n",
      "duplicate: peng2021mass\n",
      "duplicate: kitaura2017different\n",
      "duplicate: briney2019commonality\n",
      "duplicate: schroeder2006similarity\n",
      "duplicate: bondt2021human\n",
      "duplicate: bondt2021direct\n",
      "duplicate: mckinney2010data\n",
      "duplicate: walt2011numpy\n",
      "duplicate: virtanen2020scipy\n",
      "duplicate: hunter2007matplotlib:\n",
      "duplicate: schroeder2010structure\n",
      "duplicate: kaplon2021antibodies\n",
      "duplicate: marks2020how\n",
      "duplicate: raybould2020thera-sabdab:\n",
      "duplicate: bornholdt2016isolation\n",
      "duplicate: corti2016protective\n",
      "duplicate: valgardsdottir2021identification\n",
      "duplicate: schroederjr.2006similarity\n",
      "duplicate: briney2019commonality\n",
      "duplicate: hom2022exploring\n",
      "duplicate: altelaar2013next-generation\n",
      "duplicate: aebersold2016mass-spectrometric\n",
      "duplicate: sen2017automated\n",
      "duplicate: peng2021mass\n",
      "duplicate: srzentić2020interlaboratory\n",
      "duplicate: aebersold2003mass\n",
      "duplicate: tran2016complete\n",
      "duplicate: guthals2012shotgun\n",
      "duplicate: charlesajaneway2001generation\n",
      "duplicate: alberts2002generation\n",
      "duplicate: castellana2010template\n",
      "duplicate: lefranc2003imgt\n",
      "duplicate: lefranc2020immunoglobulins\n",
      "duplicate: toby2016progress\n",
      "duplicate: johansson2008ides:\n",
      "duplicate: bondt2021human\n",
      "duplicate: dupré2021de\n",
      "duplicate: bondt2021direct\n",
      "duplicate: ma2003peaks:\n",
      "duplicate: wolf2022antibody\n",
      "duplicate: raybould2020thera-sabdab:\n",
      "duplicate: guthals2017de\n",
      "duplicate: tabb2008directag:\n",
      "duplicate: he2018protein\n",
      "duplicate: boer2020selectivity\n",
      "duplicate: dupré2021de\n",
      "duplicate: brodbelt2016ion\n",
      "duplicate: schulte2022template-based\n",
      "duplicate: ma2003peaks:\n",
      "duplicate: peng2021mass\n",
      "duplicate: castellana2010template\n",
      "duplicate: georgiou2014promise\n",
      "duplicate: bondt2021human\n",
      "duplicate: lefranc2003imgt\n",
      "duplicate: shaw2020direct\n",
      "duplicate: tran2016complete\n",
      "duplicate: brodbelt2016ion\n",
      "duplicate: peng2021mass\n",
      "duplicate: peng2021mass\n",
      "duplicate: guthals2017de\n",
      "duplicate: lavinder2015next-generation\n",
      "1\n",
      "\\autoref{fig:fig2.2}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.2}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.2}\n",
      "2\n",
      "E\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.2}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.2}\n",
      "2\n",
      "D\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.3}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.3}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.3}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.4}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.4}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs2.1}\n",
      "2\n",
      "A-D\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.4}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.4}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs2.1}\n",
      "2\n",
      "E\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig2.4}\n",
      "2\n",
      "D\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.1}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.1}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.1}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.1}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.1}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.1}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.1}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.1}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.1}\n",
      "2\n",
      "D\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.1}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.1}\n",
      "2\n",
      "D\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.2}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.2}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.2}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.4}\n",
      "2\n",
      "A\n",
      "4\n",
      " and \n",
      "5\n",
      "B\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "A–D\n",
      "4\n",
      " and \n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.4}\n",
      "2\n",
      "A\n",
      "4\n",
      " and \n",
      "5\n",
      "B\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "C–E\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "C–E\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "C–E\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "C–E\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.3}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.1}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.3}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.3}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.3}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.3}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.4}\n",
      "2\n",
      "B\n",
      "4\n",
      " and \n",
      "5\n",
      "C\n",
      "1\n",
      "\\autoref{fig:fig3.5}\n",
      "2\n",
      "A\n",
      "4\n",
      " and \n",
      "5\n",
      "B\n",
      "1\n",
      "\\autoref{fig:fig3.5}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.5}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.5}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.5}\n",
      "2\n",
      "C\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.3}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.1}\n",
      "2\n",
      "B\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:figs3.1}\n",
      "2\n",
      "D\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "1\n",
      "\\autoref{fig:fig3.2}\n",
      "2\n",
      "A\n",
      "4\n",
      "None\n",
      "5\n",
      "None\n",
      "duplicate: raybould2020thera-sabdab:\n"
     ]
    }
   ],
   "source": [
    "def space_before_citation(text):\n",
    "    temp_text = text.replace(\"et al.\", \"USEDTOBEETAL\")\n",
    "    citation_regex = r'\\\\cite{[^}]+}'\n",
    "    regex = r\"([\\.,]*)[ ]*({})[ ]*([\\.,])*[ ]*\".format(citation_regex)\n",
    "    subst = \" \\\\2\\\\3\\\\1 \"\n",
    "    temp_text = re.sub(regex, subst, temp_text)\n",
    "    temp_text = temp_text.replace(\"USEDTOBEETAL\", \"et al.\")\n",
    "    # temp_text = temp_text.replace(\"\\n \", \"\\n\")\n",
    "    # temp_text = temp_text.replace(\" \\n\", \"\\n\")\n",
    "    badmatches = re.match(\"{}[^., )]\".format(citation_regex), temp_text)\n",
    "    badmatches2 = re.match(\"[^ ]{}\".format(citation_regex), temp_text)\n",
    "    if badmatches is not None or badmatches2 is not None:\n",
    "        print(\"found at least one strange match\")\n",
    "\n",
    "    return temp_text\n",
    "\n",
    "def fix_missed_emph(text):\n",
    "    to_fix = [\n",
    "        \"de novo\",\n",
    "        \"de-novo\",\n",
    "        \"m/z\"\n",
    "    ]\n",
    "    \n",
    "    regex_a = r\"(?<!\\\\emph{)\"\n",
    "    regex_b = \"(?<!})\"\n",
    "\n",
    "    for error in to_fix:\n",
    "        formatted_regex = regex_a + error + regex_b\n",
    "        formatted_replacement = r\"\\\\emph{\" + error + \"}\"\n",
    "        text = re.sub(formatted_regex, formatted_replacement, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def fix_panel_references_capitalization(text):\n",
    "    def regex_convert(m):\n",
    "        to_return = \"\"\n",
    "        to_skip = [0, 3]\n",
    "        to_lower = [2, 5]\n",
    "        for i in range(6):\n",
    "            if i in to_skip:\n",
    "                continue\n",
    "            print(i)\n",
    "            print(m.group(i))\n",
    "            if m.group(i) is not None:\n",
    "                to_add = m.group(i)\n",
    "                if i in to_lower:\n",
    "                    to_add = to_add.lower()\n",
    "                to_return = to_return + to_add\n",
    "        return to_return\n",
    "    \n",
    "    regex = r\"(\\\\autoref{[^}]+})([A-Z\\-–]+)(( and )?([A-Z])?)\"\n",
    "\n",
    "    return re.sub(regex, lambda m: regex_convert(m), text)\n",
    "\n",
    "def fix_panel_naming_capitalization(text):\n",
    "    regex = r\"(?<!Oxidation )(?<!constant )(?<!joining )(?<!diversity )(?<!Constant )(?<!Joining )(?<!Diversity )(?<! or)(?<! and)[ \\(]([a-mA-M]\\)) ?\"\n",
    "\n",
    "    return re.sub(regex, lambda m: \" ~~\" + m.group(1).lower().strip() + \" \", text)\n",
    "\n",
    "def fix_common_errors(text):\n",
    "    common_errors = [\n",
    "        (\"\\n \", \"\\n\"), # space after enter\n",
    "        (\" \\n\", \"\\n\"), # space before enter\n",
    "        (\"{ \", \"{\"), # space after {\n",
    "        (\" }\", \"}\"), # space before }\n",
    "        (\"  \", \" \"), # double space\n",
    "    ]\n",
    "\n",
    "    for error in common_errors:\n",
    "        regex = error[0]\n",
    "        replacement = error[1]\n",
    "\n",
    "        text = re.sub(regex, replacement, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_txt(folder):\n",
    "    path = folder + raw_text_filename + \".final\"\n",
    "    f = open(path, \"r\", encoding=\"utf8\")\n",
    "    text=f.read()\n",
    "    f.close()\n",
    "\n",
    "    w = open(path, \"w\", encoding=\"utf8\")\n",
    "    text = space_before_citation(text)\n",
    "    text = fix_missed_emph(text)\n",
    "    text = fix_panel_references_capitalization(text)\n",
    "    text = fix_panel_naming_capitalization(text)\n",
    "    text = fix_common_errors(text)\n",
    "    w.write(text)\n",
    "    w.close()\n",
    "\n",
    "target = base_path + \"\\\\\" + bib_merge_filename\n",
    "override = base_path + \"\\\\\" + bib_override_filename\n",
    "\n",
    "for folder_name in folders:\n",
    "    folder_path = base_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "    merge_missed(folder_path)\n",
    "\n",
    "\n",
    "merge_multiple(folders, target, override)\n",
    "process_bib(base_path + \"\\\\\")\n",
    "move_bib(base_path + \"\\\\\", target_path + \"\\\\\")\n",
    "\n",
    "for folder_name in folders:\n",
    "    folder_path = base_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "    target_path2 = target_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "    process_txt(folder_path)\n",
    "    clean_txt(folder_path)\n",
    "    move_tex(folder_path, target_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each: Variable (V), Diversity (D), Joining (J), and Constant (C),\n",
      "our genes each: Variable (V), Diversity (D), Joining (J), and Constant (C), with the light \n",
      "lambda (IGL) variable (V) 2-14$\\star$01 (IMGT/LIGM-DB: Z73664), IGL joining (J) 2$\\star$01 (IMGT/LIGM-DB: M15641), and IGL constant (C) 2$\\star$01 (IMGT/LIGM-DB: J00253) alleles. For the heavy-chain Fd portion, we determined that it was constructed from the immunoglobulin heavy (IGH) V3-9$\\star$01 (IMGT/LIGM-DB: M99651), IGHJ5$\\star$01 (IMGT/LIGM-DB: J00256), and IGHG1$\\star$03 (IMGT/LIGM-DB: Y14737) alleles and a diversity (D)\n",
      "\\caption{\\textbf{Study Workflow.} ~~a) Human mons. ~~b) SIgA1 waly. ~~c) Illust\n",
      "\\end{figure*}\n",
      "\n",
      "\\caption{\n",
      "   \\textbf{Quantitation with Cross-ID.}  ~~a) Dt alpha).  ~~b) Ligand-free and  ~~c) cAMPbounded structures of bovine alpha type I.  ~~d) \n",
      "   \n",
      "wn in  ~~e) .  ~~d) Pie \n",
      "Structuls.\n",
      " }\n",
      "\n",
      "\\textbf{Integrative \\emph{de novo} s}  ~~a) Datasis.  ~~b) Al.  ~~c) Align\n",
      "\n",
      "reduction). ~~b) F reduction). ~~c) Fragment. ~~d) data.\n",
      "\n",
      "dweeb) \n",
      "\n",
      "with the letter ~~a) \n",
      "\n",
      "(haoispa hoi) ~~a) \n",
      "(N or C)\n",
      "(N and C)\n",
      "([{)][^\\n)]+)\\(?[a-mA-M]\\)\n",
      "\n",
      "Oxidation (M),\n"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "# the above tag defines encoding for this document and is for Python 2.x compatibility\n",
    "\n",
    "import re\n",
    "regex = r\"(?<!Oxidation )(?<!constant )(?<!joining )(?<!diversity )(?<!Constant )(?<!Joining )(?<!Diversity )(?<! or)(?<! and)[ \\(]([a-mA-M]\\)) ?\"\n",
    "\n",
    "f = open(\"test.txt\", \"r\", encoding=\"utf8\")\n",
    "test_str = f.read()\n",
    "f.close()\n",
    "\n",
    "# print(test_str)\n",
    "\n",
    "# subst = r\"\\\\emph{\" + error + \"}\"\n",
    "\n",
    "# You can manually specify the number of replacements by changing the 4th argument\n",
    "\n",
    "result = re.sub(regex, lambda m: \" ~~\" + m.group(1).lower().strip() + \" \", test_str)\n",
    "\n",
    "if result:\n",
    "    print (result)\n",
    "\n",
    "# Note: for Python 2.7 compatibility, use ur\"\" to prefix the regex and u\"\" to prefix the test string and substitution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "folder_name = r\"chapter.1\"\n",
    "folder_path = base_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "target_path2 = target_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "\n",
    "mapping_ch1 = {\n",
    "}\n",
    "\n",
    "merge_missed(folder_path)\n",
    "process_txt(folder_path, mapping_ch1)\n",
    "process_bib(folder_path)\n",
    "move(folder_path, target_path2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "folder_name = r\"chapter.2\"\n",
    "folder_path = base_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "target_path2 = target_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "\n",
    "mapping_ch2 = {\n",
    "}\n",
    "\n",
    "merge_missed(folder_path)\n",
    "process_txt(folder_path, mapping_ch2)\n",
    "process_bib(folder_path)\n",
    "move(folder_path, target_path2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "folder_name = r\"chapter.3\"\n",
    "folder_path = base_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "target_path2 = target_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "\n",
    "mapping_ch3 = {\n",
    "}\n",
    "\n",
    "merge_missed(folder_path)\n",
    "process_txt(folder_path, mapping_ch3)\n",
    "process_bib(folder_path)\n",
    "move(folder_path, target_path2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "folder_name = r\"chapter.6\"\n",
    "folder_path = base_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "target_path2 = target_path + \"\\\\\" + folder_name + \"\\\\\"\n",
    "\n",
    "mapping_ch6 = {\n",
    "}\n",
    "\n",
    "merge_missed(folder_path)\n",
    "process_txt(folder_path, mapping_ch6)\n",
    "process_bib(folder_path)\n",
    "move(folder_path, target_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utility stuff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dic = {}\n",
    "\n",
    "for path in bibtexPaths:\n",
    "    f = open(path, \"r\", encoding=\"utf8\")\n",
    "    text=f.read()\n",
    "    splits = text.split(\"\\n@\")[1:]\n",
    "    for split in splits:\n",
    "        #print(split)\n",
    "        identifier = split.split(\"\\n\")[0]\n",
    "        if identifier in dic:\n",
    "            print(identifier)\n",
    "        dic[identifier.split(\"{\")[1][:-1]] = split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cit = \"Georgiou2014The promise\"\n",
    "s = cit.lower()\n",
    "newCit = re.findall(r\"[^\\d]+\\d+[^,]+\", s)[0]\n",
    "citName = re.findall(r\"[^\\d]+\", newCit)[0]\n",
    "citTitle = re.findall(r\"\\d+.+\", newCit)[0]\n",
    "pats_name = [\n",
    "    \"^de \",\n",
    "    \"^den \",\n",
    "    \"^van \",\n",
    "    \" \",\n",
    "    ]\n",
    "pats_title = [\n",
    "    \"^(\\d+)a \",\n",
    "    \"^(\\d+)the \",\n",
    "    ]\n",
    "for pat in pats_name:\n",
    "    citName = re.sub(pat, \"\", citName)\n",
    "for pat in pats_title:\n",
    "    citTitle = re.sub(pat, \"\\\\1\", citTitle)\n",
    "newCit = citName + citTitle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.sub(\"^(\\d+)the \", \"\\\\1\", citTitle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean_citation(cit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idList = []\n",
    "for path in bibtexPaths:\n",
    "    f = open(path, \"r\", encoding=\"utf8\")\n",
    "    writeLog = path + \".log\"\n",
    "    wL = open(writeLog, \"w\", encoding=\"utf8\")\n",
    "    text=f.read()\n",
    "    hits = re.findall(r\"@.+\\n\", text)\n",
    "    for id in hits:\n",
    "        text = text.replace(id, id.replace(\" \", \"\")\n",
    "        .replace(\",,\", \",\"))\n",
    "        print(id)\n",
    "\n",
    "    wL.write(text)\n",
    "    wL.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapping = {\n",
    "\t\"Charles A Janeway2001\" : \"CharlesAJaneway2001\",\n",
    "\t\"Dupré2021\" : \"Dupre2021\",\n",
    "\t\"Guthals2012\" : \"Guthals2012Shotgun\",\n",
    "\t\"Horn2000\" : \"Horn2000Automated\",\n",
    "\t\"Liu2014\" : \"Liu2014De\",\n",
    "\t\"Lössl2014\" : \"Lossl2014\",\n",
    "\t\"Marie Paule Lefranc2003\" : \"Lefranc2003\",\n",
    "\t\"Marie-Paule Lefranc2020\" : \"Lefranc2020\",\n",
    "\t\"Savidor2017\" : \"Savidor2017Database-independent\",\n",
    "\t\"Schroeder Jr.2006\" : \"SchroederJr.2006\",\n",
    "\t\"Srzentić2020\" : \"Srzentic2020\",\n",
    "\t\"Surjit Singh2018\" : \"Singh2018\",\n",
    "\t\"Vaibhav Singh2013\" : \"Singh2013\",\n",
    "\t\"de Costa2010\" : \"DeCosta2010\",\n",
    "\t\"de Haan2020\" : \"DeHaan2020\",\n",
    "\t\"den Boer2020\" : \"DenBoer2020\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done = []\n",
    "\n",
    "final = path + \".final\"\n",
    "with open(final, 'w', encoding=\"utf8\") as f:\n",
    "    for identifier in citeList:\n",
    "        if identifier not in done:\n",
    "            if identifier in dic:\n",
    "                f.write('@')\n",
    "                f.write(dic[identifier])\n",
    "                f.write(\"\\n\\n\")\n",
    "            else:\n",
    "                if identifier in mapping:\n",
    "                    f.write('@')\n",
    "                    f.write(dic[mapping[identifier]].replace(mapping[identifier], identifier.replace(' ', '')))\n",
    "                    f.write(\"\\n\\n\")\n",
    "                else:\n",
    "                    print(\"\\t\\\"{}\\\" : ,\".format(identifier))\n",
    "                \n",
    "            done.append(identifier)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "citeList"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1de8277067d0e1465e68aa6f7046d9d417af28e36ac5c57a400a18d12c493a57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
